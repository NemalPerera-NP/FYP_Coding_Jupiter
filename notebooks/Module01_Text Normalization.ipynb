{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f24d0fa8-1623-455e-ba5a-28ea09b93c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f013a65f-fa81-48ce-8465-72396fbebecf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "code is running\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "df = pd.read_csv('../data/new_tokenized_data.csv')\n",
    "print(\"code is running\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c7344c4f-fc7a-4f3d-9765-f981621289f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column Names in the Dataset:::::::\n",
      "Index(['Area_tokens', 'Popular Facilities_tokens', 'Description_tokens',\n",
      "       'Facilities_tokens', 'Surroundings_tokens', 'Bathroom Features_tokens',\n",
      "       'Bedroom Features_tokens', 'Outdoors_tokens', 'Room Amenities_tokens',\n",
      "       'Activities_tokens', 'Living Area_tokens', 'Media & Technology_tokens',\n",
      "       'Food & Drink_tokens', 'Parking_tokens', 'Reception services_tokens',\n",
      "       'Entertainment and family services_tokens', 'Cleaning services_tokens',\n",
      "       'Safety & security_tokens', 'General_tokens', 'Accessibility_tokens',\n",
      "       'Wellness_tokens', 'Languages spoken_tokens',\n",
      "       'Restaurants & cafes_tokens', 'Top attractions_tokens',\n",
      "       'Natural beauty_tokens', 'Beaches in the neighbourhood_tokens',\n",
      "       'Public transport_tokens', 'Closest airports_tokens',\n",
      "       'Cancellation/Prepayment_tokens', 'Children and Bed Policies_tokens',\n",
      "       'Age Restriction_tokens', 'Pets_tokens',\n",
      "       'Accepted Payment Methods_tokens', 'Reviews_tokens'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Print the column names\n",
    "print(\"Column Names in the Dataset:::::::\")\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2e3390d3-1c9f-4b1d-ab90-e3b4c6a23ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Steps for Text Normalization\n",
    "# Lemmatization: Converts each token into its base or dictionary form.\n",
    "# Case Normalization: Typically converts all text to lowercase to standardize the data.\n",
    "# Removing Stop Words: Filters out common words that might not be useful in the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ad802a2d-34df-4cbf-b142-3b4017cd6729",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f8d73f8b-af8c-4319-8ddc-ed4152887234",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatized data successfully saved to ../data/lemmatized_tokenized_data.csv\n",
      "Columns lemmatized and saved: Index(['Area_tokens_lemmatized', 'Popular Facilities_tokens_lemmatized',\n",
      "       'Description_tokens_lemmatized', 'Facilities_tokens_lemmatized',\n",
      "       'Surroundings_tokens_lemmatized', 'Bathroom Features_tokens_lemmatized',\n",
      "       'Bedroom Features_tokens_lemmatized', 'Outdoors_tokens_lemmatized',\n",
      "       'Room Amenities_tokens_lemmatized', 'Activities_tokens_lemmatized',\n",
      "       'Living Area_tokens_lemmatized', 'Media & Technology_tokens_lemmatized',\n",
      "       'Food & Drink_tokens_lemmatized', 'Parking_tokens_lemmatized',\n",
      "       'Reception services_tokens_lemmatized',\n",
      "       'Entertainment and family services_tokens_lemmatized',\n",
      "       'Cleaning services_tokens_lemmatized',\n",
      "       'Safety & security_tokens_lemmatized', 'General_tokens_lemmatized',\n",
      "       'Accessibility_tokens_lemmatized', 'Wellness_tokens_lemmatized',\n",
      "       'Languages spoken_tokens_lemmatized',\n",
      "       'Restaurants & cafes_tokens_lemmatized',\n",
      "       'Top attractions_tokens_lemmatized', 'Natural beauty_tokens_lemmatized',\n",
      "       'Beaches in the neighbourhood_tokens_lemmatized',\n",
      "       'Public transport_tokens_lemmatized',\n",
      "       'Closest airports_tokens_lemmatized',\n",
      "       'Cancellation/Prepayment_tokens_lemmatized',\n",
      "       'Children and Bed Policies_tokens_lemmatized',\n",
      "       'Age Restriction_tokens_lemmatized', 'Pets_tokens_lemmatized',\n",
      "       'Accepted Payment Methods_tokens_lemmatized',\n",
      "       'Reviews_tokens_lemmatized'],\n",
      "      dtype='object')\n",
      "  Area_tokens_lemmatized               Popular Facilities_tokens_lemmatized  \\\n",
      "0              [unknown]  [private, beach, outdoor, swimming, pool, airp...   \n",
      "1                 [ella]  [airport, shuttle, non, smoking, room, room, s...   \n",
      "2            [hikkaduwa]  [outdoor, swimming, pool, airport, shuttle, no...   \n",
      "3                [kandy]  [free, wifi, family, room, free, parking, rest...   \n",
      "4              [unknown]  [outdoor, swimming, pool, airport, shuttle, no...   \n",
      "\n",
      "                       Description_tokens_lemmatized  \\\n",
      "0  [108, 108, 3, 6, you, re, eligible, for, a, ge...   \n",
      "1  [3, 2.6, km, nine, 3, 49, km, 24, hour, 3, 1.4...   \n",
      "2  [33, 1.8, km, seenigama, beach, 33, 5, 24, hou...   \n",
      "3  [360, 3, 1.4, km, continental, asian, kandy, r...   \n",
      "4  [3r, less, than, 1, km, 3r, 4, 2.2, km, 3r, ma...   \n",
      "\n",
      "                        Facilities_tokens_lemmatized  \\\n",
      "0  [bathroom, bathroom, bedroom, bedroom, view, v...   \n",
      "1  [bathroom, bathroom, bedroom, bedroom, view, v...   \n",
      "2  [bathroom, bathroom, bedroom, bedroom, view, v...   \n",
      "3  [outdoors, outdoors, food, drink, food, drink,...   \n",
      "4  [bathroom, bathroom, bedroom, bedroom, view, v...   \n",
      "\n",
      "                      Surroundings_tokens_lemmatized  \\\n",
      "0  [what, s, nearby, restaurant, cafe, beach, in,...   \n",
      "1  [what, s, nearby, restaurant, cafe, natural, b...   \n",
      "2  [what, s, nearby, restaurant, cafe, top, attra...   \n",
      "3  [what, s, nearby, restaurant, cafe, top, attra...   \n",
      "4  [what, s, nearby, restaurant, cafe, beach, in,...   \n",
      "\n",
      "                 Bathroom Features_tokens_lemmatized  \\\n",
      "0  [toilet, paper, towel, private, bathroom, toil...   \n",
      "1  [toilet, paper, towel, bidet, towelssheet, ext...   \n",
      "2  [toilet, paper, towel, bidet, towelssheet, ext...   \n",
      "3                                          [unknown]   \n",
      "4  [toilet, paper, towel, bath, or, shower, priva...   \n",
      "\n",
      "                  Bedroom Features_tokens_lemmatized  \\\n",
      "0                      [linen, wardrobe, or, closet]   \n",
      "1  [2, metre, linen, wardrobe, or, closet, extra,...   \n",
      "2  [2, metre, linen, wardrobe, or, closet, dressi...   \n",
      "3                                          [unknown]   \n",
      "4      [linen, wardrobe, or, closet, dressing, room]   \n",
      "\n",
      "                          Outdoors_tokens_lemmatized  \\\n",
      "0  [private, beach, outdoor, fireplace, picnic, a...   \n",
      "1  [outdoor, fireplace, picnic, area, outdoor, fu...   \n",
      "2  [picnic, area, outdoor, furniture, outdoor, di...   \n",
      "3                      [outdoor, furniture, terrace]   \n",
      "4        [bbq, facility, additional, charge, garden]   \n",
      "\n",
      "                    Room Amenities_tokens_lemmatized  \\\n",
      "0  [socket, near, the, bed, dry, rack, for, cloth...   \n",
      "1  [socket, near, the, bed, dry, rack, for, cloth...   \n",
      "2  [socket, near, the, bed, dry, rack, for, cloth...   \n",
      "3                                          [unknown]   \n",
      "4                           [socket, near, the, bed]   \n",
      "\n",
      "                        Activities_tokens_lemmatized  ...  \\\n",
      "0  [3, km, bicycle, rental, additional, charge, a...  ...   \n",
      "1  [bicycle, rental, additional, charge, cooking,...  ...   \n",
      "2  [cook, class, additional, charge, tour, or, cl...  ...   \n",
      "3                                          [unknown]  ...   \n",
      "4                                          [unknown]  ...   \n",
      "\n",
      "          Natural beauty_tokens_lemmatized  \\\n",
      "0                                [unknown]   \n",
      "1     [mountainsingle, tree, hill, 33, km]   \n",
      "2  [44, km, seaoceanwhale, watch, mirissa]   \n",
      "3     [mountainsingle, tree, hill, 39, km]   \n",
      "4                                [unknown]   \n",
      "\n",
      "      Beaches in the neighbourhood_tokens_lemmatized  \\\n",
      "0  [sampalthivu, beach, 20, uppuveli, beach, 1.5,...   \n",
      "1                                          [unknown]   \n",
      "2  [seenigama, beach, 1.4, km, hikkaduwa, beach, ...   \n",
      "3                                          [unknown]   \n",
      "4  [negombo, beach, 350, poruthota, beach, 1.5, k...   \n",
      "\n",
      "                  Public transport_tokens_lemmatized  \\\n",
      "0  [traintrincomalee, 6, km, trainchina, bay, 9, ...   \n",
      "1  [2.6, km, trainella, railway, station, 900, m,...   \n",
      "2  [trainhikkaduwa, 1.2, km, 1.3, km, 1.7, km, ra...   \n",
      "3  [trainkandy, railway, station, 1.2, km, 800, m...   \n",
      "4  [450, 2.8, km, trainkattuwa, m, trainkochchikade]   \n",
      "\n",
      "                  Closest airports_tokens_lemmatized  \\\n",
      "0  [china, bay, airport, 10, km, 92, km, sigiriya...   \n",
      "1  [rajapaksa, international, airport, 64, km, ma...   \n",
      "2  [koggala, airport, 29, km, ratmalana, internat...   \n",
      "3  [74, km, bandaranaike, international, airport,...   \n",
      "4  [bandaranaike, international, airport, 7, km, ...   \n",
      "\n",
      "           Cancellation/Prepayment_tokens_lemmatized  \\\n",
      "0  [cancellation, and, prepayment, policy, vary, ...   \n",
      "1  [cancellation, and, prepayment, policy, vary, ...   \n",
      "2  [cancellation, and, prepayment, policy, vary, ...   \n",
      "3  [cancellation, and, prepayment, policy, vary, ...   \n",
      "4  [cancellation, and, prepayment, policy, vary, ...   \n",
      "\n",
      "         Children and Bed Policies_tokens_lemmatized  \\\n",
      "0  [13, year, child, policy, child, of, any, age,...   \n",
      "1  [old, than, 1, year, 7, year, 1, 2, year, 6, 1...   \n",
      "2  [child, policie, child, of, any, age, be, welc...   \n",
      "3  [child, policie, child, of, any, age, be, welc...   \n",
      "4  [child, policie, child, of, any, age, be, welc...   \n",
      "\n",
      "                   Age Restriction_tokens_lemmatized  \\\n",
      "0  [there, be, no, age, requirement, for, check, in]   \n",
      "1  [no, age, restriction, for, check, in, only, c...   \n",
      "2  [there, be, no, age, requirement, for, check, in]   \n",
      "3  [there, be, no, age, requirement, for, check, in]   \n",
      "4  [there, be, no, age, requirement, for, check, in]   \n",
      "\n",
      "                      Pets_tokens_lemmatized  \\\n",
      "0  [free, pet, be, allow, no, extra, charge]   \n",
      "1                      [pet, be, not, allow]   \n",
      "2                      [pet, be, not, allow]   \n",
      "3                      [pet, be, not, allow]   \n",
      "4                      [pet, be, not, allow]   \n",
      "\n",
      "          Accepted Payment Methods_tokens_lemmatized  \\\n",
      "0                                          [unknown]   \n",
      "1  [maestro, mastercard, jcb, american, visa, exp...   \n",
      "2                                 [mastercard, visa]   \n",
      "3                                          [unknown]   \n",
      "4                                 [mastercard, visa]   \n",
      "\n",
      "                           Reviews_tokens_lemmatized  \n",
      "0  [nilaveli, one, asia, 50, one, sri, lanka, sri...  \n",
      "1                                                 []  \n",
      "2                                                 []  \n",
      "3                                                 []  \n",
      "4  [awesome, beach, dutch, pastel, first, 5, in, ...  \n",
      "\n",
      "[5 rows x 34 columns]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import ast  # To safely evaluate strings that look like lists\n",
    "\n",
    "# Load the spaCy English language model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def parse_list_from_string(list_string):\n",
    "    try:\n",
    "        # Safely evaluate strings that represent lists\n",
    "        return ast.literal_eval(list_string)\n",
    "    except:\n",
    "        # Return the original string if it's not a list-like string\n",
    "        return list_string.split() if isinstance(list_string, str) else list_string\n",
    "\n",
    "def lemmatize_tokens(tokens):\n",
    "    \"\"\"\n",
    "    Lemmatize a list of tokens using spaCy.\n",
    "    Args:\n",
    "    tokens (list of str): The tokens to lemmatize.\n",
    "    Returns:\n",
    "    list of str: The lemmatized tokens.\n",
    "    \"\"\"\n",
    "    # Convert list of tokens back to text\n",
    "    text = ' '.join(tokens)\n",
    "    \n",
    "    # Define the maximum chunk size\n",
    "    max_chunk_size = 1000000  # Maximum number of characters per chunk\n",
    "    \n",
    "    # Split the text into manageable chunks\n",
    "    chunks = [text[i:i + max_chunk_size] for i in range(0, len(text), max_chunk_size)]\n",
    "    \n",
    "    lemmatized_tokens = []\n",
    "    try:\n",
    "        for chunk in chunks:\n",
    "            doc = nlp(chunk)\n",
    "            # Extract lemmas for each token in the chunk\n",
    "            lemmatized_tokens.extend([token.lemma_ for token in doc if not token.is_punct and not token.is_space])\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during lemmatization: {e}\")\n",
    "    \n",
    "    return lemmatized_tokens\n",
    "\n",
    "def apply_lemmatization(data, token_columns):\n",
    "    \"\"\"\n",
    "    Apply lemmatization to tokenized columns of the dataframe and return only lemmatized columns.\n",
    "    Args:\n",
    "    data (DataFrame): The DataFrame containing the tokenized data.\n",
    "    token_columns (list): A list of column names containing tokenized data.\n",
    "    Returns:\n",
    "    DataFrame: A new DataFrame containing only the lemmatized columns.\n",
    "    \"\"\"\n",
    "    lemmatized_data = pd.DataFrame()\n",
    "    for column in token_columns:\n",
    "        if column in data.columns:\n",
    "            # Ensure that the tokens are parsed as lists\n",
    "            data[column] = data[column].apply(parse_list_from_string)\n",
    "            lemmatized_data[column + '_lemmatized'] = data[column].apply(lemmatize_tokens)\n",
    "    return lemmatized_data\n",
    "\n",
    "# Load your data (ensure this path matches where your data is stored)\n",
    "# df = pd.read_csv('../data/your_tokenized_data.csv')\n",
    "\n",
    "# Identify tokenized columns (assuming these are named with a '_tokens' suffix)\n",
    "token_columns = [col for col in df.columns if '_tokens' in col]\n",
    "\n",
    "# Apply lemmatization and retrieve only lemmatized columns\n",
    "lemmatized_data = apply_lemmatization(df, token_columns)\n",
    "\n",
    "# Save the lemmatized data\n",
    "save_path = '../data/lemmatized_tokenized_data.csv'\n",
    "directory = os.path.dirname(save_path)\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "\n",
    "try:\n",
    "    lemmatized_data.to_csv(save_path, index=False)\n",
    "    print(f\"Lemmatized data successfully saved to {save_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while saving the lemmatized data: {e}\")\n",
    "\n",
    "# Preview the lemmatized data\n",
    "print(\"Columns lemmatized and saved:\", lemmatized_data.columns)\n",
    "print(lemmatized_data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db308ebf-90a3-4811-8f2b-8aa088fb4469",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step-by-Step Automated Testing Code\n",
    "# Setup: Load your dataset and the spaCy language model.\n",
    "# Validation Function: Create a function to compare the lemmatized tokens in the dataset against the lemmas generated by spaCy from the original text.\n",
    "# Applying the Validation: Apply this function across your DataFrame to check each row.\n",
    "# Report Results: Summarize and report any discrepancies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce459924-b876-4d95-b6c3-0de9e073c737",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Lemmatized Tokens'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\Desktop\\L4 - Research\\Coding\\Develpment Environment\\module01\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Lemmatized Tokens'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 31\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m expected_lemmas \u001b[38;5;241m==\u001b[39m lemmatized_tokens\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# Assuming your DataFrame has a column 'Original Text' and 'Lemmatized Tokens' that needs to be validated\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# Convert lemmatized tokens stored as string back to list\u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLemmatized Tokens\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mLemmatized Tokens\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28meval\u001b[39m)\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# Apply the validation function\u001b[39;00m\n\u001b[0;32m     34\u001b[0m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mValidation Result\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m row: validate_lemmatization(row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOriginal Text\u001b[39m\u001b[38;5;124m'\u001b[39m], row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLemmatized Tokens\u001b[39m\u001b[38;5;124m'\u001b[39m]), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\Desktop\\L4 - Research\\Coding\\Develpment Environment\\module01\\Lib\\site-packages\\pandas\\core\\frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32m~\\Desktop\\L4 - Research\\Coding\\Develpment Environment\\module01\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3810\u001b[0m     ):\n\u001b[0;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Lemmatized Tokens'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "\n",
    "# Load the spaCy English language model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Load your lemmatized dataset\n",
    "data = pd.read_csv('../data/lemmatized_tokenized_data.csv')\n",
    "\n",
    "def validate_lemmatization(original_text, lemmatized_tokens):\n",
    "    \"\"\"\n",
    "    Validate lemmatization by comparing the dataset's lemmatized tokens against spaCy's output.\n",
    "    \n",
    "    Args:\n",
    "    original_text (str): The original text before tokenization and lemmatization.\n",
    "    lemmatized_tokens (list of str): Lemmatized tokens from the dataset.\n",
    "\n",
    "    Returns:\n",
    "    bool: True if the validation passes, False otherwise.\n",
    "    \"\"\"\n",
    "    # Process the original text with spaCy\n",
    "    doc = nlp(original_text)\n",
    "    # Generate expected lemmas, filtering out punctuation and spaces\n",
    "    expected_lemmas = [token.lemma_ for token in doc if not token.is_punct and not token.is_space]\n",
    "\n",
    "    # Compare the expected lemmas with the lemmatized tokens from the dataset\n",
    "    return expected_lemmas == lemmatized_tokens\n",
    "\n",
    "# Assuming your DataFrame has a column 'Original Text' and 'Lemmatized Tokens' that needs to be validated\n",
    "# Convert lemmatized tokens stored as string back to list\n",
    "data['Lemmatized Tokens'] = data['Lemmatized Tokens'].apply(eval)\n",
    "\n",
    "# Apply the validation function\n",
    "data['Validation Result'] = data.apply(lambda row: validate_lemmatization(row['Original Text'], row['Lemmatized Tokens']), axis=1)\n",
    "\n",
    "# Check for rows where validation failed\n",
    "invalid_rows = data[data['Validation Result'] == False]\n",
    "print(f\"Number of rows with invalid lemmatization: {len(invalid_rows)}\")\n",
    "\n",
    "if len(invalid_rows) > 0:\n",
    "    print(invalid_rows[['Original Text', 'Lemmatized Tokens']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5271aa13-c56f-4de0-823b-c539ba63ecf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python Code to Apply Case Normalization and Stop Word Removal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "462d491e-dd2f-40ec-ae82-7b2a7264ae49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns processed and cleaned: ['Area_tokens_lemmatized', 'Popular Facilities_tokens_lemmatized', 'Description_tokens_lemmatized', 'Facilities_tokens_lemmatized', 'Surroundings_tokens_lemmatized', 'Bathroom Features_tokens_lemmatized', 'Bedroom Features_tokens_lemmatized', 'Outdoors_tokens_lemmatized', 'Room Amenities_tokens_lemmatized', 'Activities_tokens_lemmatized', 'Living Area_tokens_lemmatized', 'Media & Technology_tokens_lemmatized', 'Food & Drink_tokens_lemmatized', 'Parking_tokens_lemmatized', 'Reception services_tokens_lemmatized', 'Entertainment and family services_tokens_lemmatized', 'Cleaning services_tokens_lemmatized', 'Safety & security_tokens_lemmatized', 'General_tokens_lemmatized', 'Accessibility_tokens_lemmatized', 'Wellness_tokens_lemmatized', 'Languages spoken_tokens_lemmatized', 'Restaurants & cafes_tokens_lemmatized', 'Top attractions_tokens_lemmatized', 'Natural beauty_tokens_lemmatized', 'Beaches in the neighbourhood_tokens_lemmatized', 'Public transport_tokens_lemmatized', 'Closest airports_tokens_lemmatized', 'Cancellation/Prepayment_tokens_lemmatized', 'Children and Bed Policies_tokens_lemmatized', 'Age Restriction_tokens_lemmatized', 'Pets_tokens_lemmatized', 'Accepted Payment Methods_tokens_lemmatized', 'Reviews_tokens_lemmatized']\n",
      "  Area_tokens_lemmatized               Popular Facilities_tokens_lemmatized  \\\n",
      "0              [unknown]  [private, beach, outdoor, swimming, pool, airp...   \n",
      "1                 [ella]  [airport, shuttle, non, smoking, room, room, s...   \n",
      "2            [hikkaduwa]  [outdoor, swimming, pool, airport, shuttle, no...   \n",
      "3                [kandy]  [free, wifi, family, room, free, parking, rest...   \n",
      "4              [unknown]  [outdoor, swimming, pool, airport, shuttle, no...   \n",
      "\n",
      "                       Description_tokens_lemmatized  \\\n",
      "0  [108, 108, 3, 6, you, re, eligible, for, a, ge...   \n",
      "1  [3, 2.6, km, nine, 3, 49, km, 24, hour, 3, 1.4...   \n",
      "2  [33, 1.8, km, seenigama, beach, 33, 5, 24, hou...   \n",
      "3  [360, 3, 1.4, km, continental, asian, kandy, r...   \n",
      "4  [3r, less, than, 1, km, 3r, 4, 2.2, km, 3r, ma...   \n",
      "\n",
      "                        Facilities_tokens_lemmatized  \\\n",
      "0  [bathroom, bathroom, bedroom, bedroom, view, v...   \n",
      "1  [bathroom, bathroom, bedroom, bedroom, view, v...   \n",
      "2  [bathroom, bathroom, bedroom, bedroom, view, v...   \n",
      "3  [outdoors, outdoors, food, drink, food, drink,...   \n",
      "4  [bathroom, bathroom, bedroom, bedroom, view, v...   \n",
      "\n",
      "                      Surroundings_tokens_lemmatized  \\\n",
      "0  [what, s, nearby, restaurant, cafe, beach, in,...   \n",
      "1  [what, s, nearby, restaurant, cafe, natural, b...   \n",
      "2  [what, s, nearby, restaurant, cafe, top, attra...   \n",
      "3  [what, s, nearby, restaurant, cafe, top, attra...   \n",
      "4  [what, s, nearby, restaurant, cafe, beach, in,...   \n",
      "\n",
      "                 Bathroom Features_tokens_lemmatized  \\\n",
      "0  [toilet, paper, towel, private, bathroom, toil...   \n",
      "1  [toilet, paper, towel, bidet, towelssheet, ext...   \n",
      "2  [toilet, paper, towel, bidet, towelssheet, ext...   \n",
      "3                                          [unknown]   \n",
      "4  [toilet, paper, towel, bath, or, shower, priva...   \n",
      "\n",
      "                  Bedroom Features_tokens_lemmatized  \\\n",
      "0                      [linen, wardrobe, or, closet]   \n",
      "1  [2, metre, linen, wardrobe, or, closet, extra,...   \n",
      "2  [2, metre, linen, wardrobe, or, closet, dressi...   \n",
      "3                                          [unknown]   \n",
      "4      [linen, wardrobe, or, closet, dressing, room]   \n",
      "\n",
      "                          Outdoors_tokens_lemmatized  \\\n",
      "0  [private, beach, outdoor, fireplace, picnic, a...   \n",
      "1  [outdoor, fireplace, picnic, area, outdoor, fu...   \n",
      "2  [picnic, area, outdoor, furniture, outdoor, di...   \n",
      "3                      [outdoor, furniture, terrace]   \n",
      "4        [bbq, facility, additional, charge, garden]   \n",
      "\n",
      "                    Room Amenities_tokens_lemmatized  \\\n",
      "0  [socket, near, the, bed, dry, rack, for, cloth...   \n",
      "1  [socket, near, the, bed, dry, rack, for, cloth...   \n",
      "2  [socket, near, the, bed, dry, rack, for, cloth...   \n",
      "3                                          [unknown]   \n",
      "4                           [socket, near, the, bed]   \n",
      "\n",
      "                        Activities_tokens_lemmatized  ...  \\\n",
      "0  [3, km, bicycle, rental, additional, charge, a...  ...   \n",
      "1  [bicycle, rental, additional, charge, cooking,...  ...   \n",
      "2  [cook, class, additional, charge, tour, or, cl...  ...   \n",
      "3                                          [unknown]  ...   \n",
      "4                                          [unknown]  ...   \n",
      "\n",
      "    Natural beauty_tokens_lemmatized_clean  \\\n",
      "0                                [unknown]   \n",
      "1     [mountainsingle, tree, hill, 33, km]   \n",
      "2  [44, km, seaoceanwhale, watch, mirissa]   \n",
      "3     [mountainsingle, tree, hill, 39, km]   \n",
      "4                                [unknown]   \n",
      "\n",
      "  Beaches in the neighbourhood_tokens_lemmatized_clean  \\\n",
      "0  [sampalthivu, beach, 20, uppuveli, beach, 1.5,...     \n",
      "1                                          [unknown]     \n",
      "2  [seenigama, beach, 1.4, km, hikkaduwa, beach, ...     \n",
      "3                                          [unknown]     \n",
      "4  [negombo, beach, 350, poruthota, beach, 1.5, k...     \n",
      "\n",
      "            Public transport_tokens_lemmatized_clean  \\\n",
      "0  [traintrincomalee, 6, km, trainchina, bay, 9, ...   \n",
      "1  [2.6, km, trainella, railway, station, 900, m,...   \n",
      "2  [trainhikkaduwa, 1.2, km, 1.3, km, 1.7, km, ra...   \n",
      "3  [trainkandy, railway, station, 1.2, km, 800, m...   \n",
      "4  [450, 2.8, km, trainkattuwa, m, trainkochchikade]   \n",
      "\n",
      "            Closest airports_tokens_lemmatized_clean  \\\n",
      "0  [china, bay, airport, 10, km, 92, km, sigiriya...   \n",
      "1  [rajapaksa, international, airport, 64, km, ma...   \n",
      "2  [koggala, airport, 29, km, ratmalana, internat...   \n",
      "3  [74, km, bandaranaike, international, airport,...   \n",
      "4  [bandaranaike, international, airport, 7, km, ...   \n",
      "\n",
      "     Cancellation/Prepayment_tokens_lemmatized_clean  \\\n",
      "0  [cancellation, prepayment, policy, vary, accor...   \n",
      "1  [cancellation, prepayment, policy, vary, accor...   \n",
      "2  [cancellation, prepayment, policy, vary, accor...   \n",
      "3  [cancellation, prepayment, policy, vary, accor...   \n",
      "4  [cancellation, prepayment, policy, vary, accor...   \n",
      "\n",
      "   Children and Bed Policies_tokens_lemmatized_clean  \\\n",
      "0  [13, year, child, policy, child, age, welcome,...   \n",
      "1  [old, 1, year, 7, year, 1, 2, year, 6, 11, yea...   \n",
      "2  [child, policie, child, age, welcome, correct,...   \n",
      "3  [child, policie, child, age, welcome, correct,...   \n",
      "4  [child, policie, child, age, welcome, correct,...   \n",
      "\n",
      "           Age Restriction_tokens_lemmatized_clean  \\\n",
      "0                        [age, requirement, check]   \n",
      "1  [age, restriction, check, child, 1, old, allow]   \n",
      "2                        [age, requirement, check]   \n",
      "3                        [age, requirement, check]   \n",
      "4                        [age, requirement, check]   \n",
      "\n",
      "        Pets_tokens_lemmatized_clean  \\\n",
      "0  [free, pet, allow, extra, charge]   \n",
      "1                       [pet, allow]   \n",
      "2                       [pet, allow]   \n",
      "3                       [pet, allow]   \n",
      "4                       [pet, allow]   \n",
      "\n",
      "    Accepted Payment Methods_tokens_lemmatized_clean  \\\n",
      "0                                          [unknown]   \n",
      "1  [maestro, mastercard, jcb, american, visa, exp...   \n",
      "2                                 [mastercard, visa]   \n",
      "3                                          [unknown]   \n",
      "4                                 [mastercard, visa]   \n",
      "\n",
      "                     Reviews_tokens_lemmatized_clean  \n",
      "0  [nilaveli, asia, 50, sri, lanka, sri, lanka, e...  \n",
      "1                                                 []  \n",
      "2                                                 []  \n",
      "3                                                 []  \n",
      "4  [awesome, beach, dutch, pastel, 5, morning, 9,...  \n",
      "\n",
      "[5 rows x 68 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "import ast\n",
    "\n",
    "# Load the spaCy English language model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def parse_tokens_from_string(list_string):\n",
    "    \"\"\"\n",
    "    Parse and safely evaluate strings that look like list of tokens.\n",
    "    Args:\n",
    "    list_string (str): String representation of a list of tokens.\n",
    "    \n",
    "    Returns:\n",
    "    list: Evaluated list of tokens if valid, otherwise an empty list.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Safely evaluate string that represents a list\n",
    "        tokens = ast.literal_eval(list_string)\n",
    "        if isinstance(tokens, list):\n",
    "            return tokens\n",
    "        else:\n",
    "            return []\n",
    "    except:\n",
    "        # In case of any error, return an empty list\n",
    "        return []\n",
    "\n",
    "def normalize_and_remove_stopwords(tokens):\n",
    "    \"\"\"\n",
    "    Normalize case and remove stop words from a list of tokens.\n",
    "    Args:\n",
    "    tokens (list of str): The tokens to process.\n",
    "    \n",
    "    Returns:\n",
    "    list of str: Tokens after converting to lowercase and removing stop words.\n",
    "    \"\"\"\n",
    "    # Convert list of tokens back to text for processing\n",
    "    text = ' '.join(tokens)\n",
    "    \n",
    "    # Process the text with spaCy\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # Apply case normalization and filter out stop words and punctuation\n",
    "    filtered_tokens = [token.text.lower() for token in doc if not token.is_stop and not token.is_punct and not token.is_space]\n",
    "    \n",
    "    return filtered_tokens\n",
    "\n",
    "def apply_text_processing(data, token_columns):\n",
    "    \"\"\"\n",
    "    Apply text processing including normalization and stop word removal to specified token columns of the dataframe.\n",
    "    Args:\n",
    "    data (DataFrame): The DataFrame containing the tokenized data.\n",
    "    token_columns (list): A list of column names containing tokenized data.\n",
    "    \n",
    "    Returns:\n",
    "    DataFrame: The DataFrame with text processed columns.\n",
    "    \"\"\"\n",
    "    for column in token_columns:\n",
    "        if column in data.columns:\n",
    "            # Parse tokens if they are stored as string representations of lists\n",
    "            data[column] = data[column].apply(parse_tokens_from_string)\n",
    "            data[column + '_clean'] = data[column].apply(normalize_and_remove_stopwords)\n",
    "    return data\n",
    "\n",
    "# Load your lemmatized data\n",
    "df = pd.read_csv('../data/lemmatized_tokenized_data.csv')\n",
    "\n",
    "# Identify lemmatized columns (assuming these are named with a '_lemmatized' suffix)\n",
    "lemmatized_columns = [col for col in df.columns if '_lemmatized' in col]\n",
    "\n",
    "# Apply text processing\n",
    "df = apply_text_processing(df, lemmatized_columns)\n",
    "\n",
    "# Save the processed data\n",
    "#df.to_csv('../data/cleaned_data.csv', index=False)\n",
    "\n",
    "# Preview the cleaned data\n",
    "print(\"Columns processed and cleaned:\", lemmatized_columns)\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd19c45-efcd-4bfc-a69c-6637e902f005",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# To update the code so that you can save two different datasets—one with all the data and another with only the cleaned data columns—follow the steps \n",
    "# below. This approach will let you maintain a complete set with the original and processed data and a separate file that contains only the cleaned \n",
    "# columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7735ef81-0ef2-4b7f-823d-e7c96ce88aa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns processed and cleaned: ['Area_tokens_lemmatized', 'Popular Facilities_tokens_lemmatized', 'Description_tokens_lemmatized', 'Facilities_tokens_lemmatized', 'Surroundings_tokens_lemmatized', 'Bathroom Features_tokens_lemmatized', 'Bedroom Features_tokens_lemmatized', 'Outdoors_tokens_lemmatized', 'Room Amenities_tokens_lemmatized', 'Activities_tokens_lemmatized', 'Living Area_tokens_lemmatized', 'Media & Technology_tokens_lemmatized', 'Food & Drink_tokens_lemmatized', 'Parking_tokens_lemmatized', 'Reception services_tokens_lemmatized', 'Entertainment and family services_tokens_lemmatized', 'Cleaning services_tokens_lemmatized', 'Safety & security_tokens_lemmatized', 'General_tokens_lemmatized', 'Accessibility_tokens_lemmatized', 'Wellness_tokens_lemmatized', 'Languages spoken_tokens_lemmatized', 'Restaurants & cafes_tokens_lemmatized', 'Top attractions_tokens_lemmatized', 'Natural beauty_tokens_lemmatized', 'Beaches in the neighbourhood_tokens_lemmatized', 'Public transport_tokens_lemmatized', 'Closest airports_tokens_lemmatized', 'Cancellation/Prepayment_tokens_lemmatized', 'Children and Bed Policies_tokens_lemmatized', 'Age Restriction_tokens_lemmatized', 'Pets_tokens_lemmatized', 'Accepted Payment Methods_tokens_lemmatized', 'Reviews_tokens_lemmatized']\n",
      "  Area_tokens_lemmatized_clean  \\\n",
      "0                    [unknown]   \n",
      "1                       [ella]   \n",
      "2                  [hikkaduwa]   \n",
      "3                      [kandy]   \n",
      "4                    [unknown]   \n",
      "\n",
      "          Popular Facilities_tokens_lemmatized_clean  \\\n",
      "0  [private, beach, outdoor, swimming, pool, airp...   \n",
      "1  [airport, shuttle, non, smoking, room, room, s...   \n",
      "2  [outdoor, swimming, pool, airport, shuttle, no...   \n",
      "3  [free, wifi, family, room, free, parking, rest...   \n",
      "4  [outdoor, swimming, pool, airport, shuttle, no...   \n",
      "\n",
      "                 Description_tokens_lemmatized_clean  \\\n",
      "0  [108, 108, 3, 6, eligible, genius, discount, p...   \n",
      "1  [3, 2.6, km, 3, 49, km, 24, hour, 3, 1.4, km, ...   \n",
      "2  [33, 1.8, km, seenigama, beach, 33, 5, 24, hou...   \n",
      "3  [360, 3, 1.4, km, continental, asian, kandy, r...   \n",
      "4  [3r, 1, km, 3r, 4, 2.2, km, 3r, maris, stella,...   \n",
      "\n",
      "                  Facilities_tokens_lemmatized_clean  \\\n",
      "0  [bathroom, bathroom, bedroom, bedroom, view, v...   \n",
      "1  [bathroom, bathroom, bedroom, bedroom, view, v...   \n",
      "2  [bathroom, bathroom, bedroom, bedroom, view, v...   \n",
      "3  [outdoors, outdoors, food, drink, food, drink,...   \n",
      "4  [bathroom, bathroom, bedroom, bedroom, view, v...   \n",
      "\n",
      "                Surroundings_tokens_lemmatized_clean  \\\n",
      "0  [s, nearby, restaurant, cafe, beach, neighbour...   \n",
      "1  [s, nearby, restaurant, cafe, natural, beauty,...   \n",
      "2  [s, nearby, restaurant, cafe, attraction, natu...   \n",
      "3  [s, nearby, restaurant, cafe, attraction, natu...   \n",
      "4  [s, nearby, restaurant, cafe, beach, neighbour...   \n",
      "\n",
      "           Bathroom Features_tokens_lemmatized_clean  \\\n",
      "0  [toilet, paper, towel, private, bathroom, toil...   \n",
      "1  [toilet, paper, towel, bidet, towelssheet, ext...   \n",
      "2  [toilet, paper, towel, bidet, towelssheet, ext...   \n",
      "3                                          [unknown]   \n",
      "4  [toilet, paper, towel, bath, shower, private, ...   \n",
      "\n",
      "            Bedroom Features_tokens_lemmatized_clean  \\\n",
      "0                          [linen, wardrobe, closet]   \n",
      "1  [2, metre, linen, wardrobe, closet, extra, lon...   \n",
      "2  [2, metre, linen, wardrobe, closet, dressing, ...   \n",
      "3                                          [unknown]   \n",
      "4          [linen, wardrobe, closet, dressing, room]   \n",
      "\n",
      "                    Outdoors_tokens_lemmatized_clean  \\\n",
      "0  [private, beach, outdoor, fireplace, picnic, a...   \n",
      "1  [outdoor, fireplace, picnic, area, outdoor, fu...   \n",
      "2  [picnic, area, outdoor, furniture, outdoor, di...   \n",
      "3                      [outdoor, furniture, terrace]   \n",
      "4        [bbq, facility, additional, charge, garden]   \n",
      "\n",
      "              Room Amenities_tokens_lemmatized_clean  \\\n",
      "0  [socket, near, bed, dry, rack, clothing, cloth...   \n",
      "1  [socket, near, bed, dry, rack, clothing, cloth...   \n",
      "2  [socket, near, bed, dry, rack, clothing, cloth...   \n",
      "3                                          [unknown]   \n",
      "4                                [socket, near, bed]   \n",
      "\n",
      "                  Activities_tokens_lemmatized_clean  ...  \\\n",
      "0  [3, km, bicycle, rental, additional, charge, a...  ...   \n",
      "1  [bicycle, rental, additional, charge, cooking,...  ...   \n",
      "2  [cook, class, additional, charge, tour, class,...  ...   \n",
      "3                                          [unknown]  ...   \n",
      "4                                          [unknown]  ...   \n",
      "\n",
      "    Natural beauty_tokens_lemmatized_clean  \\\n",
      "0                                [unknown]   \n",
      "1     [mountainsingle, tree, hill, 33, km]   \n",
      "2  [44, km, seaoceanwhale, watch, mirissa]   \n",
      "3     [mountainsingle, tree, hill, 39, km]   \n",
      "4                                [unknown]   \n",
      "\n",
      "  Beaches in the neighbourhood_tokens_lemmatized_clean  \\\n",
      "0  [sampalthivu, beach, 20, uppuveli, beach, 1.5,...     \n",
      "1                                          [unknown]     \n",
      "2  [seenigama, beach, 1.4, km, hikkaduwa, beach, ...     \n",
      "3                                          [unknown]     \n",
      "4  [negombo, beach, 350, poruthota, beach, 1.5, k...     \n",
      "\n",
      "            Public transport_tokens_lemmatized_clean  \\\n",
      "0  [traintrincomalee, 6, km, trainchina, bay, 9, ...   \n",
      "1  [2.6, km, trainella, railway, station, 900, m,...   \n",
      "2  [trainhikkaduwa, 1.2, km, 1.3, km, 1.7, km, ra...   \n",
      "3  [trainkandy, railway, station, 1.2, km, 800, m...   \n",
      "4  [450, 2.8, km, trainkattuwa, m, trainkochchikade]   \n",
      "\n",
      "            Closest airports_tokens_lemmatized_clean  \\\n",
      "0  [china, bay, airport, 10, km, 92, km, sigiriya...   \n",
      "1  [rajapaksa, international, airport, 64, km, ma...   \n",
      "2  [koggala, airport, 29, km, ratmalana, internat...   \n",
      "3  [74, km, bandaranaike, international, airport,...   \n",
      "4  [bandaranaike, international, airport, 7, km, ...   \n",
      "\n",
      "     Cancellation/Prepayment_tokens_lemmatized_clean  \\\n",
      "0  [cancellation, prepayment, policy, vary, accor...   \n",
      "1  [cancellation, prepayment, policy, vary, accor...   \n",
      "2  [cancellation, prepayment, policy, vary, accor...   \n",
      "3  [cancellation, prepayment, policy, vary, accor...   \n",
      "4  [cancellation, prepayment, policy, vary, accor...   \n",
      "\n",
      "   Children and Bed Policies_tokens_lemmatized_clean  \\\n",
      "0  [13, year, child, policy, child, age, welcome,...   \n",
      "1  [old, 1, year, 7, year, 1, 2, year, 6, 11, yea...   \n",
      "2  [child, policie, child, age, welcome, correct,...   \n",
      "3  [child, policie, child, age, welcome, correct,...   \n",
      "4  [child, policie, child, age, welcome, correct,...   \n",
      "\n",
      "           Age Restriction_tokens_lemmatized_clean  \\\n",
      "0                        [age, requirement, check]   \n",
      "1  [age, restriction, check, child, 1, old, allow]   \n",
      "2                        [age, requirement, check]   \n",
      "3                        [age, requirement, check]   \n",
      "4                        [age, requirement, check]   \n",
      "\n",
      "        Pets_tokens_lemmatized_clean  \\\n",
      "0  [free, pet, allow, extra, charge]   \n",
      "1                       [pet, allow]   \n",
      "2                       [pet, allow]   \n",
      "3                       [pet, allow]   \n",
      "4                       [pet, allow]   \n",
      "\n",
      "    Accepted Payment Methods_tokens_lemmatized_clean  \\\n",
      "0                                          [unknown]   \n",
      "1  [maestro, mastercard, jcb, american, visa, exp...   \n",
      "2                                 [mastercard, visa]   \n",
      "3                                          [unknown]   \n",
      "4                                 [mastercard, visa]   \n",
      "\n",
      "                     Reviews_tokens_lemmatized_clean  \n",
      "0  [nilaveli, asia, 50, sri, lanka, sri, lanka, e...  \n",
      "1                                                 []  \n",
      "2                                                 []  \n",
      "3                                                 []  \n",
      "4  [awesome, beach, dutch, pastel, 5, morning, 9,...  \n",
      "\n",
      "[5 rows x 34 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "import ast\n",
    "\n",
    "# Load the spaCy English language model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def parse_tokens_from_string(list_string):\n",
    "    \"\"\"\n",
    "    Parse and safely evaluate strings that look like list of tokens.\n",
    "    Args:\n",
    "    list_string (str): String representation of a list of tokens.\n",
    "    \n",
    "    Returns:\n",
    "    list: Evaluated list of tokens if valid, otherwise an empty list.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Safely evaluate string that represents a list\n",
    "        tokens = ast.literal_eval(list_string)\n",
    "        if isinstance(tokens, list):\n",
    "            return tokens\n",
    "        else:\n",
    "            return []\n",
    "    except:\n",
    "        # In case of any error, return an empty list\n",
    "        return []\n",
    "\n",
    "def normalize_and_remove_stopwords(tokens):\n",
    "    \"\"\"\n",
    "    Normalize case and remove stop words from a list of tokens.\n",
    "    Args:\n",
    "    tokens (list of str): The tokens to process.\n",
    "    \n",
    "    Returns:\n",
    "    list of str: Tokens after converting to lowercase and removing stop words.\n",
    "    \"\"\"\n",
    "    # Convert list of tokens back to text for processing\n",
    "    text = ' '.join(tokens)\n",
    "    \n",
    "    # Process the text with spaCy\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # Apply case normalization and filter out stop words and punctuation\n",
    "    filtered_tokens = [token.text.lower() for token in doc if not token.is_stop and not token.is_punct and not token.is_space]\n",
    "    \n",
    "    return filtered_tokens\n",
    "\n",
    "def apply_text_processing(data, token_columns):\n",
    "    \"\"\"\n",
    "    Apply text processing including normalization and stop word removal to specified token columns of the dataframe.\n",
    "    Args:\n",
    "    data (DataFrame): The DataFrame containing the tokenized data.\n",
    "    token_columns (list): A list of column names containing tokenized data.\n",
    "    \n",
    "    Returns:\n",
    "    DataFrame: The DataFrame with text processed columns.\n",
    "    \"\"\"\n",
    "    for column in token_columns:\n",
    "        if column in data.columns:\n",
    "            # Parse tokens if they are stored as string representations of lists\n",
    "            data[column] = data[column].apply(parse_tokens_from_string)\n",
    "            data[column + '_clean'] = data[column].apply(normalize_and_remove_stopwords)\n",
    "    return data\n",
    "\n",
    "# Load your lemmatized data\n",
    "df = pd.read_csv('../data/lemmatized_tokenized_data.csv')\n",
    "\n",
    "# Identify lemmatized columns (assuming these are named with a '_lemmatized' suffix)\n",
    "lemmatized_columns = [col for col in df.columns if '_lemmatized' in col]\n",
    "\n",
    "# Apply text processing\n",
    "df = apply_text_processing(df, lemmatized_columns)\n",
    "\n",
    "# Save the complete dataset with both original and cleaned data\n",
    "df.to_csv('../data/complete_processed_stopword_data.csv', index=False)\n",
    "\n",
    "# Create and save a dataset with only the cleaned columns\n",
    "cleaned_columns = [col for col in df.columns if '_clean' in col]\n",
    "df_cleaned = df[cleaned_columns]\n",
    "df_cleaned.to_csv('../data/only_cleaned_data.csv', index=False)\n",
    "\n",
    "# Preview the cleaned data\n",
    "print(\"Columns processed and cleaned:\", lemmatized_columns)\n",
    "print(df[cleaned_columns].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f9c48b-e990-46ca-9eb6-fa89616e3407",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
