{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "940879e1-d231-4d06-9445-159be28eabea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization for Ontology Population\n",
    "# This notebook focuses on the tokenization of hotel reviews to aid in the identification \n",
    "# of key phrases and terms that are relevant for ontology population."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "72b4067f-f5f2-4221-8ae3-7de38a1a69f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a8c73906-b04f-4f08-8cb0-5be249bd9343",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'tokenized_columns' in locals():\n",
    "    del tokenized_columns\n",
    "if 'data' in locals():\n",
    "    del data\n",
    "if 'df' in locals():\n",
    "    del df\n",
    "if 'new_data' in locals():\n",
    "    del new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "73a8ed80-2bb3-4a2c-835e-14134a4cbc5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "code is running\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "df = pd.read_csv('../data/merged_dataset.csv')\n",
    "print(\"code is running\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "363e26a5-99cc-4686-b02e-05d0e33a14bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column Names in the Dataset New:\n",
      "Index(['Area', 'Hotel Name', 'Hotel Address', 'Popular Facilities',\n",
      "       'Description', 'Facilities', 'Surroundings', 'Bathroom Features',\n",
      "       'Bedroom Features', 'Outdoors', 'Room Amenities', 'Activities',\n",
      "       'Living Area', 'Media & Technology', 'Food & Drink', 'Parking',\n",
      "       'Reception services', 'Entertainment and family services',\n",
      "       'Cleaning services', 'Safety & security', 'General', 'Accessibility',\n",
      "       'Wellness', 'Languages spoken', 'Restaurants & cafes',\n",
      "       'Top attractions', 'Natural beauty', 'Beaches in the neighbourhood',\n",
      "       'Public transport', 'Closest airports', 'Check-in', 'Check-out',\n",
      "       'Cancellation/Prepayment', 'Children and Bed Policies',\n",
      "       'Age Restriction', 'Pets', 'Accepted Payment Methods', 'Rating Value',\n",
      "       'Reviews'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Print the column names\n",
    "print(\"Column Names in the Dataset New:\")\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d5a2a664-511c-41bd-baa7-2278291fc9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here's the modified code that:\n",
    "\n",
    "# Excludes certain columns from being tokenized by default.\n",
    "# Applies tokenization using the list of columns deemed suitable after the automatic detection, excluding specific columns.\n",
    "# Saves the tokenized data and prints a preview.\n",
    "\n",
    "\n",
    "# This setup ensures that:\n",
    "\n",
    "# You explicitly avoid tokenizing columns that shouldn't be split, like 'Hotel Name' and 'Hotel Address'.\n",
    "# You apply tokenization only to suitable columns detected automatically but filtered through your exclusion list.\n",
    "# You can utilize the results immediately in further data processing or analysis steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8ab531f4-cf1d-41bf-90aa-cf8e3eb47a2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully saved to ../data/new_tokenized_data.csv\n",
      "Columns tokenized: ['Area', 'Popular Facilities', 'Description', 'Facilities', 'Surroundings', 'Bathroom Features', 'Bedroom Features', 'Outdoors', 'Room Amenities', 'Activities', 'Living Area', 'Media & Technology', 'Food & Drink', 'Parking', 'Reception services', 'Entertainment and family services', 'Cleaning services', 'Safety & security', 'General', 'Accessibility', 'Wellness', 'Languages spoken', 'Restaurants & cafes', 'Top attractions', 'Natural beauty', 'Beaches in the neighbourhood', 'Public transport', 'Closest airports', 'Cancellation/Prepayment', 'Children and Bed Policies', 'Age Restriction', 'Pets', 'Accepted Payment Methods', 'Reviews']\n",
      "   Area_tokens                          Popular Facilities_tokens  \\\n",
      "0    [unknown]  [private beach, outdoor, swimming, pool, airpo...   \n",
      "1       [ella]  [airport, shuttle, non, smoking, rooms, room, ...   \n",
      "2  [hikkaduwa]  [outdoor, swimming, pool, airport, shuttle, no...   \n",
      "3      [kandy]  [free, wifi, family, rooms, free, parking, res...   \n",
      "4    [unknown]  [outdoor, swimming, pool, airport, shuttle, no...   \n",
      "\n",
      "                                  Description_tokens  \\\n",
      "0  [108, 108, 3, 6, you, re, eligible, for, a, ge...   \n",
      "1  [3, 2.6 km, nine, 3, 49 km, 24-hour, 3, 1.4 km...   \n",
      "2  [33, 1.8 km, seenigama beach, 33, 5, 24-hour, ...   \n",
      "3  [360, 3, 1.4 km, continental, asian, kandy rai...   \n",
      "4  [3r, less than 1 km, 3r, 4, 2.2 km, 3r, maris ...   \n",
      "\n",
      "                                   Facilities_tokens  \\\n",
      "0  [bathroom, bathroom, bedroom, bedroom, view, v...   \n",
      "1  [bathroom, bathroom, bedroom, bedroom, view, v...   \n",
      "2  [bathroom, bathroom, bedroom, bedroom, view, v...   \n",
      "3  [outdoors, outdoors, food, drink, food, drink,...   \n",
      "4  [bathroom, bathroom, bedroom, bedroom, view, v...   \n",
      "\n",
      "                                 Surroundings_tokens  \\\n",
      "0  [what, s, nearby, restaurants, cafes, beaches,...   \n",
      "1  [what, s, nearby, restaurants, cafes, natural,...   \n",
      "2  [what, s, nearby, restaurants, cafes, top, att...   \n",
      "3  [what, s, nearby, restaurants, cafes, top, att...   \n",
      "4  [what, s, nearby, restaurants, cafes, beaches,...   \n",
      "\n",
      "                            Bathroom Features_tokens  \\\n",
      "0  [toilet, paper, towels, private, bathroom, toi...   \n",
      "1  [toilet, paper, towels, bidet, towelssheets, e...   \n",
      "2  [toilet, paper, towels, bidet, towelssheets, e...   \n",
      "3                                          [unknown]   \n",
      "4  [toilet, paper, towels, bath, or, shower, priv...   \n",
      "\n",
      "                             Bedroom Features_tokens  \\\n",
      "0                      [linen, wardrobe, or, closet]   \n",
      "1  [2 metres, linen, wardrobe, or, closet, extra,...   \n",
      "2  [2 metres, linen, wardrobe, or, closet, dressi...   \n",
      "3                                          [unknown]   \n",
      "4      [linen, wardrobe, or, closet, dressing, room]   \n",
      "\n",
      "                                     Outdoors_tokens  \\\n",
      "0  [private beach, outdoor, fireplace, picnic, ar...   \n",
      "1  [outdoor, fireplace, picnic, area, outdoor, fu...   \n",
      "2  [picnic, area, outdoor, furniture, outdoor, di...   \n",
      "3                      [outdoor, furniture, terrace]   \n",
      "4       [bbq facilities, additional, charge, garden]   \n",
      "\n",
      "                               Room Amenities_tokens  \\\n",
      "0  [socket, near, the, bed, drying, rack, for, cl...   \n",
      "1  [socket, near, the, bed, drying, rack, for, cl...   \n",
      "2  [socket, near, the, bed, drying, rack, for, cl...   \n",
      "3                                          [unknown]   \n",
      "4                           [socket, near, the, bed]   \n",
      "\n",
      "                                   Activities_tokens  ...  \\\n",
      "0  [3 km, bicycle, rental, additional, charge, ae...  ...   \n",
      "1  [bicycle, rental, additional, charge, cooking,...  ...   \n",
      "2  [cooking, class, additional, charge, tour, or,...  ...   \n",
      "3                                          [unknown]  ...   \n",
      "4                                          [unknown]  ...   \n",
      "\n",
      "                       Natural beauty_tokens  \\\n",
      "0                                  [unknown]   \n",
      "1          [mountainsingle tree hill, 33 km]   \n",
      "2  [44 km, seaoceanwhale, watching, mirissa]   \n",
      "3          [mountainsingle tree hill, 39 km]   \n",
      "4                                  [unknown]   \n",
      "\n",
      "                 Beaches in the neighbourhood_tokens  \\\n",
      "0  [sampalthivu beach, 20, uppuveli beach, 1.5 km...   \n",
      "1                                          [unknown]   \n",
      "2  [seenigama beach, 1.4 km, hikkaduwa beach, 1.5...   \n",
      "3                                          [unknown]   \n",
      "4  [negombo beach, 350, poruthota beach, 1.5 km, ...   \n",
      "\n",
      "                             Public transport_tokens  \\\n",
      "0  [traintrincomalee, 6 km, trainchina bay, 9 km,...   \n",
      "1  [2.6 km, trainella, railway, station, 900, m, ...   \n",
      "2  [trainhikkaduwa, 1.2 km, 1.3 km, 1.7 km, railw...   \n",
      "3  [trainkandy railway station, 1.2 km, 800, m, t...   \n",
      "4   [450, 2.8 km, trainkattuwa, m, trainkochchikade]   \n",
      "\n",
      "                             Closest airports_tokens  \\\n",
      "0  [china bay airport, 10 km, 92 km, sigiriya, ai...   \n",
      "1  [rajapaksa international airport, 64 km, mattala]   \n",
      "2  [koggala airport, 29 km, ratmalana internation...   \n",
      "3  [74 km, bandaranaike international airport, 83...   \n",
      "4  [bandaranaike international airport, 7 km, rat...   \n",
      "\n",
      "                      Cancellation/Prepayment_tokens  \\\n",
      "0  [cancellation, and, prepayment, policies, vary...   \n",
      "1  [cancellation, and, prepayment, policies, vary...   \n",
      "2  [cancellation, and, prepayment, policies, vary...   \n",
      "3  [cancellation, and, prepayment, policies, vary...   \n",
      "4  [cancellation, and, prepayment, policies, vary...   \n",
      "\n",
      "                    Children and Bed Policies_tokens  \\\n",
      "0  [13 years, child, policies, children, of, any,...   \n",
      "1  [older than 1 year, 7 years, 1 - 2 years, 6 - ...   \n",
      "2  [child, policies, children, of, any, age, are,...   \n",
      "3  [child, policies, children, of, any, age, are,...   \n",
      "4  [child, policies, children, of, any, age, are,...   \n",
      "\n",
      "                              Age Restriction_tokens  \\\n",
      "0  [there, is, no, age, requirement, for, check, in]   \n",
      "1  [no, age, restriction, for, check, in, only, c...   \n",
      "2  [there, is, no, age, requirement, for, check, in]   \n",
      "3  [there, is, no, age, requirement, for, check, in]   \n",
      "4  [there, is, no, age, requirement, for, check, in]   \n",
      "\n",
      "                                      Pets_tokens  \\\n",
      "0  [free, pets, are, allowed, no, extra, charges]   \n",
      "1                       [pets, are, not, allowed]   \n",
      "2                       [pets, are, not, allowed]   \n",
      "3                       [pets, are, not, allowed]   \n",
      "4                       [pets, are, not, allowed]   \n",
      "\n",
      "                     Accepted Payment Methods_tokens  \\\n",
      "0                                          [unknown]   \n",
      "1  [maestro, mastercard, jcb, american, visa, exp...   \n",
      "2                                 [mastercard, visa]   \n",
      "3                                          [unknown]   \n",
      "4                                 [mastercard, visa]   \n",
      "\n",
      "                                      Reviews_tokens  \n",
      "0  [nilaveli, one, asia, 50, one, sri lanka, sri ...  \n",
      "1                                                 []  \n",
      "2                                                 []  \n",
      "3                                                 []  \n",
      "4  [awesome beach, dutch, pastel, first, 5 in the...  \n",
      "\n",
      "[5 rows x 34 columns]\n"
     ]
    }
   ],
   "source": [
    "# Load the spaCy English language model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def is_text_column(column, exclude_columns=[]):\n",
    "    \"\"\"\n",
    "    Heuristic to determine if a column should be tokenized based on checking\n",
    "    if more than a certain percentage of the rows contain mostly text data,\n",
    "    excluding specified columns.\n",
    "    \"\"\"\n",
    "    # Sample 100 entries or less\n",
    "    sample = column.dropna().sample(min(100, len(column)))\n",
    "    text_count = 0\n",
    "    for entry in sample:\n",
    "        # Consider it text if more than half of the characters are letters\n",
    "        if isinstance(entry, str) and sum(c.isalpha() for c in entry) / len(entry) > 0.5:\n",
    "            text_count += 1\n",
    "    # Threshold: 80% of the sample are text entries\n",
    "    return text_count > 0.8 * len(sample)\n",
    "\n",
    "def apply_tokenization(data, exclude_columns=[]):\n",
    "    \"\"\"\n",
    "    Apply tokenization to automatically detected text columns, excluding specified columns.\n",
    "    \"\"\"\n",
    "    text_columns = [col for col in data.columns if is_text_column(data[col]) and col not in exclude_columns]\n",
    "    for column in text_columns:\n",
    "        data[column + '_tokens'] = data[column].apply(lambda x: [token.text for token in nlp(str(x))] if pd.notnull(x) else [])\n",
    "    return data, text_columns\n",
    "\n",
    "def custom_tokenizer(text):\n",
    "    \"\"\"\n",
    "    Custom tokenizer using spaCy to handle named entities and regular text.\n",
    "    \"\"\"\n",
    "    doc = nlp(text)\n",
    "    tokens = []\n",
    "    for ent in doc.ents:\n",
    "        tokens.append(ent.text)  # Keep the entire entity intact.\n",
    "\n",
    "    # Tokenize the text around named entities\n",
    "    index = 0\n",
    "    for ent in doc.ents:\n",
    "        tokens.extend([token.text for token in doc[index:ent.start] if not token.is_punct and not token.is_space])\n",
    "        index = ent.end\n",
    "    tokens.extend([token.text for token in doc[index:] if not token.is_punct and not token.is_space])\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "def apply_custom_tokenization(data, columns):\n",
    "    \"\"\"\n",
    "    Apply the custom tokenizer to multiple columns of the dataframe.\n",
    "    \"\"\"\n",
    "    new_data = pd.DataFrame()\n",
    "    for column in columns:\n",
    "        if column in data.columns:\n",
    "            new_data[column + '_tokens'] = data[column].apply(lambda x: custom_tokenizer(str(x)) if pd.notnull(x) else [])\n",
    "    return new_data\n",
    "\n",
    "\n",
    "# Automatically detect text columns and apply tokenization, excluding 'Hotel Name' and 'Hotel Address'\n",
    "data, tokenized_columns = apply_tokenization(df, exclude_columns=['Hotel Name', 'Hotel Address'])\n",
    "\n",
    "# Apply custom tokenization using the detected columns and create a new dataset\n",
    "new_data = apply_custom_tokenization(data, tokenized_columns)\n",
    "\n",
    "# Check and create directory if needed, then save new dataset\n",
    "save_path = '../data/new_tokenized_data.csv'\n",
    "directory = os.path.dirname(save_path)\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "\n",
    "try:\n",
    "    new_data.to_csv(save_path, index=False)\n",
    "    print(f\"Data successfully saved to {save_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while saving the data: {e}\")\n",
    "\n",
    "# Preview the tokenized data\n",
    "print(\"Columns tokenized:\", tokenized_columns)\n",
    "print(new_data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "20cf96db-3db0-49db-a2ba-6bada9d2d80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'tokenized_columns' in locals():\n",
    "    del tokenized_columns\n",
    "if 'data' in locals():\n",
    "    del data\n",
    "if 'df' in locals():\n",
    "    del df\n",
    "if 'new_data' in locals():\n",
    "    del new_data\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
